import os
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv() 

from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, trim_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict
from typing import Sequence
from langchain_core.messages import BaseMessage

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# GPT-4o ëª¨ë¸ ì´ˆê¸°í™”
model = init_chat_model(
    "gpt-4o-mini",
    model_provider="openai",
    temperature=0.3,  # ì°½ì˜ì„± ì¤„ì´ê³ , ë” ëª…í™•í•œ ì‘ë‹µ ìœ ë„
    max_tokens=1024   # ì‘ë‹µ ê¸¸ì´ ì œí•œ
)

# ì—­í• (Role) ì •ì˜
ROLE_DESCRIPTION = """
ë‹¹ì‹ ì€ 20ë…„ ì´ìƒì˜ ê²½ë ¥ì„ ê°€ì§„ ì‹œë‹ˆì–´ ê°œë°œìì…ë‹ˆë‹¤.  
ì‚¬ìš©ìê°€ ì œê³µí•œ í•™ìŠµ ë…¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ìµœì í™”ëœ Markdown í˜•ì‹ì˜ TIL**ì„ ì‘ì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
"""

# ì‘ë‹µ ìŠ¤íƒ€ì¼(Response Style) ì„¤ì •
RESPONSE_STYLE = """
- **ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”.**  
- **Markdown í˜•ì‹ì„ ì—„ê²©í•˜ê²Œ ì¤€ìˆ˜í•˜ì„¸ìš”.**  
- **ì‚¬ìš©ìì˜ ì›ë˜ í‘œí˜„ì„ ì‚´ë¦¬ë˜, ê°€ë…ì„±ì„ ë†’ì´ë„ë¡ ê°œì„ í•˜ì„¸ìš”.**  
- **ë¶ˆí•„ìš”í•œ ë‚´ìš©ì„ ì œê±°í•˜ê³ , í•µì‹¬ ë‚´ìš©ì„ ê°•ì¡°í•˜ì„¸ìš”.**  
"""

# ê·œì¹™(Rules) ì •ì˜
RULES = """
- ğŸ“Œ **ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ Markdownìœ¼ë¡œ ì‘ì„±í•  ê²ƒ**:
## ë‚ ì§œ: YYYY-MM-DD  # ì„¤ëª…: ì‚¬ìš©ìì—ê²Œ ì…ë ¥ë°›ì€ ë‚ ì§œ ë° ìš”ì¼ì„ ë„£ì–´ì£¼ë˜, (ì›”)ê³¼ ê°™ì´ ê³µë°± ì—†ì´ ì¶œë ¥í•  ê²ƒ.
- **ë‚ ì§œ í˜•ì‹ì„ ìœ ì§€í•˜ë©°, ê´„í˜¸()ì™€ ìš”ì¼ ì‚¬ì´ì— ê³µë°±ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.**

### ğŸ“Œ ìŠ¤í¬ëŸ¼
- í•™ìŠµ ëª©í‘œ 1: (ë‚´ìš©)
- í•™ìŠµ ëª©í‘œ 2: (ë‚´ìš©)
- í•™ìŠµ ëª©í‘œ 3: (ë‚´ìš©)

### ğŸ“– ìƒˆë¡œ ë°°ìš´ ë‚´ìš©

#### ì£¼ì œ 1: (ì£¼ì œ ì„¤ëª…)
ğŸ“Œ (í•µì‹¬ ìš”ì•½)
- (ì„¸ë¶€ ë‚´ìš© 1)
- (ì„¸ë¶€ ë‚´ìš© 2)

#### ì£¼ì œ 2: (ì£¼ì œ ì„¤ëª…)
ğŸ“Œ (í•µì‹¬ ìš”ì•½)
- (ì„¸ë¶€ ë‚´ìš© 1)
- (ì„¸ë¶€ ë‚´ìš© 2)

### ğŸ¯ ì˜¤ëŠ˜ì˜ ë„ì „ ê³¼ì œì™€ í•´ê²° ë°©ë²•
- **ë„ì „ ê³¼ì œ 1**: (ì„¤ëª… ë° í•´ê²° ë°©ë²•)
- **ë„ì „ ê³¼ì œ 2**: (ì„¤ëª… ë° í•´ê²° ë°©ë²•)

### ğŸ“ ì˜¤ëŠ˜ì˜ íšŒê³ 
- (í•™ìŠµ ê²½í—˜ì— ëŒ€í•œ íšŒê³ )

### ğŸ”— ì°¸ê³  ìë£Œ ë° ë§í¬
- [ë§í¬ ì œëª©](URL)

- **Markdown ê°œí–‰(`"  \n"`)ì„ ì§€ì¼œì•¼ í•©ë‹ˆë‹¤.**  
- **ì´ì „ ëŒ€í™”ë¥¼ ì°¸ê³ í•˜ì—¬ ì¤‘ë³µì„ ì¤„ì´ê³  ë¶€ì¡±í•œ ë‚´ìš©ì„ ë³´ì™„í•˜ì„¸ìš”.**  
- **í•™ìŠµ ëª©í‘œì™€ í•´ê²° ë°©ë²•ì„ ëª…í™•í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.**  
- **ìœ ìš©í•œ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.**  
"""

# 4ï¸ì§€ì‹œë¬¸(Prompt Instructions)
PROMPT_INSTRUCTIONS = """
ì•„ë˜ ì‚¬ìš©ìì˜ í•™ìŠµ ë…¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ„ì˜ ì—­í• , ì‘ë‹µ ìŠ¤íƒ€ì¼, ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì—¬ ìµœì ì˜ TILì„ ì‘ì„±í•˜ì„¸ìš”.  
"""

# ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°ë¦½
prompt_template = ChatPromptTemplate.from_messages(
  [
      ("system", ROLE_DESCRIPTION),
      ("system", RESPONSE_STYLE),
      ("system", RULES),
      ("system", PROMPT_INSTRUCTIONS),
      MessagesPlaceholder(variable_name="messages"),
  ]
)

# ë©”ì‹œì§€ ìµœì í™” (í† í° ê¸¸ì´ ì œí•œ)
trimmer = trim_messages(
  max_tokens=512,
  strategy="last",
  token_counter=model,
  include_system=True,
  allow_partial=False,
  start_on="human",
)

# ì»¤ìŠ¤í…€ ìƒíƒœ ì •ì˜ (ì…ë ¥ ë°ì´í„° í¬í•¨)
class State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]

# LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
workflow = StateGraph(state_schema=State)

# LLM í˜¸ì¶œ í•¨ìˆ˜ (ì„±ëŠ¥ ìµœì í™” ì ìš©)
def call_model(state: State):
    trimmed_messages = trimmer.invoke(state["messages"])
    prompt = prompt_template.invoke({"messages": trimmed_messages})
    response = model.invoke(prompt)
    return {"messages": response}

# ëª¨ë¸ í˜¸ì¶œ ë…¸ë“œ ì¶”ê°€
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# LangGraph ë©”ëª¨ë¦¬ ì €ì¥ (ì„¸ì…˜ ê´€ë¦¬ ê°€ëŠ¥, ì„±ëŠ¥ ìµœì í™” ì ìš©)
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# FastAPI ì—”ë“œí¬ì¸íŠ¸ì—ì„œ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜ (ë¹„ë™ê¸° ì²˜ë¦¬ ì¶”ê°€)
async def generate_til(user_notes: str) -> str:
    input_messages = [HumanMessage(user_notes)]
    try:
        print("ğŸ“Œ LangGraph ì…ë ¥:", user_notes)
        config = {"configurable": {"thread_id": "static_session"}}  # ì„¸ì…˜ ì¬ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
        output = app.invoke({"messages": input_messages}, config=config)
        response_text = output["messages"][-1].content.replace("\n", "  \n")  # GitHub í˜¸í™˜ ê°œí–‰ ì²˜ë¦¬
        print("ğŸš€ LangGraph ì‘ë‹µ:", response_text)
        return response_text
    except Exception as e:
        print("LangGraph ì˜¤ë¥˜:", e)
        return ""