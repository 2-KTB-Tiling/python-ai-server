import os
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv() 

from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, trim_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, MessagesState, StateGraph
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph.message import add_messages
from typing_extensions import Annotated, TypedDict
from typing import Sequence
from langchain_core.messages import BaseMessage

# GPT-4o-mini ëª¨ë¸ ì´ˆê¸°í™” -> ë¹ ë¥¸ ì‘ë‹µ ì†ë„ë¥¼ ìœ„í•´ ëª¨ë¸ ë³€ê²½
model = init_chat_model("gpt-4o-mini", model_provider="openai")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (TIL ìë™ ìƒì„±)
prompt_template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """ë‹¹ì‹ ì€ 20ë…„ ì´ìƒì˜ ê²½í—˜ì„ ê°€ì§„ ì‹œë‹ˆì–´ ê°œë°œìì…ë‹ˆë‹¤.  
            ì…ë ¥ëœ ë°ì´í„°ë¥¼ ê²€í† í•˜ì—¬ ì˜ëª»ëœ ë‚´ìš©ì„ ì •ì •í•˜ê³ , ë‹¤ìŒ TIL í˜•ì‹ì— ë§ì¶°ì„œ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

            ì‘ì„± ì‹œ í•„ìˆ˜ ì¤€ìˆ˜ ì‚¬í•­
            1. í˜•ì‹œì ì¸ ì¢…ê²° ì–´ë¯¸ëŠ” ìµœëŒ€í•œ ìƒëµí•˜ê³ , ë¶ˆê°€í”¼í•œ ê²½ìš° ~ì„ ê³¼ ê°™ì´ ì§§ê²Œ ì‘ì„±í•´ ê¹”ë”í•˜ê³  êµ°ë”ë”ê¸° ì—†ì´ ì‘ì„±í•œë‹¤.
            2. ìµœëŒ€í•œ í‘œ, ë¶ˆë ›ì„ í™œìš©í•´ ê°€ì‹œì„± í–¥ìƒí•œë‹¤,

            ## ë‚ ì§œ: YYYY-MM-DD

            ### ğŸ“Œ ìŠ¤í¬ëŸ¼
            - í•™ìŠµ ëª©í‘œ 1: (ë‚´ìš©)
            - í•™ìŠµ ëª©í‘œ 2: (ë‚´ìš©)
            - í•™ìŠµ ëª©í‘œ 3: (ë‚´ìš©)

            ### ğŸ“– ìƒˆë¡œ ë°°ìš´ ë‚´ìš©

            #### ğŸ”¹ ì£¼ì œ 1: (ì£¼ì œ ì„¤ëª…)
            ğŸ“Œ (í•µì‹¬ ìš”ì•½)
            - (ì„¸ë¶€ ë‚´ìš© 1)
            - (ì„¸ë¶€ ë‚´ìš© 2)

            #### ğŸ”¹ ì£¼ì œ 2: (ì£¼ì œ ì„¤ëª…)
            ğŸ“Œ (í•µì‹¬ ìš”ì•½)
            - (ì„¸ë¶€ ë‚´ìš© 1)
            - (ì„¸ë¶€ ë‚´ìš© 2)

            ### ğŸ¯ ì˜¤ëŠ˜ì˜ ë„ì „ ê³¼ì œì™€ í•´ê²° ë°©ë²•
            - **ë„ì „ ê³¼ì œ 1**: (ì„¤ëª… ë° í•´ê²° ë°©ë²•)
            - **ë„ì „ ê³¼ì œ 2**: (ì„¤ëª… ë° í•´ê²° ë°©ë²•)

            ### ğŸ“ ì˜¤ëŠ˜ì˜ íšŒê³ 
            - (í•™ìŠµ ê²½í—˜ì— ëŒ€í•œ íšŒê³ )

            ### ğŸ”— ì°¸ê³  ìë£Œ ë° ë§í¬
            - [ë§í¬ ì œëª©](URL)
            """,
        ),
        MessagesPlaceholder(variable_name="messages"),
    ]
)

#ë©”ì‹œì§€ ìµœì í™” (í† í° ê¸¸ì´ ì œí•œ)
trimmer = trim_messages(

    max_tokens=512,  # ! ì ì ˆí•œ í† í° ìˆ˜ë¡œ ì œí•œ
    strategy="last",
    token_counter=model,
    include_system=True,
    allow_partial=False,
    start_on="human",
)

# ì»¤ìŠ¤í…€ ìƒíƒœ ì •ì˜ (ì…ë ¥ ë°ì´í„° í¬í•¨)
class State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]

# LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
workflow = StateGraph(state_schema=State)

# LLM í˜¸ì¶œ í•¨ìˆ˜
def call_model(state: State):
    trimmed_messages = trimmer.invoke(state["messages"])
    prompt = prompt_template.invoke({"messages": trimmed_messages})
    response = model.invoke(prompt)
    return {"messages": response}

# ëª¨ë¸ í˜¸ì¶œ ë…¸ë“œ ì¶”ê°€
workflow.add_edge(START, "model")
workflow.add_node("model", call_model)

# ë©”ëª¨ë¦¬ ì €ì¥ (ì„¸ì…˜ ê´€ë¦¬ ê°€ëŠ¥)
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# FastAPI ì—”ë“œí¬ì¸íŠ¸ì—ì„œ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜
def generate_til(user_notes: str) -> str:
    input_messages = [HumanMessage(user_notes)]
    try:
        print("ğŸ“Œ LangGraph ì…ë ¥:", user_notes)  # ì…ë ¥ê°’ í™•ì¸
        config = {"configurable": {"thread_id": "unique_session_id"}}  # ê³ ìœ í•œ ì„¸ì…˜ ID ì¶”ê°€
        output = app.invoke({"messages": input_messages}, config=config)  # `config` ì¶”ê°€
        response_text = output["messages"][-1].content
        print("ğŸš€ LangGraph ì‘ë‹µ:", response_text)  # ì‘ë‹µ í™•ì¸
        return response_text
    except Exception as e:
        print("LangGraph ì˜¤ë¥˜:", e)  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¶œë ¥
        return ""  # ë¹ˆ ì‘ë‹µ ë°˜í™˜
